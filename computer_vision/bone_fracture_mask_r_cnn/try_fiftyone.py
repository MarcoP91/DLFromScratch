
import os
import pandas as pd
import cv2
import time
from glob import glob
from tqdm import tqdm
import numpy as np 
import cv2 as cv
import zipfile
import io
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import warnings
from sklearn.impute import SimpleImputer
import albumentations
from sklearn.model_selection import train_test_split
from scipy.ndimage.morphology import binary_dilation
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, models, transforms
import torchvision.transforms.functional as TF
from torchvision.utils import make_grid
from pathlib import Path
import matplotlib.patches as patches
from PIL import Image

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass

warnings.warn = warn
warnings.filterwarnings('ignore')

label_names = ['elbow positive', 'fingers positive', 'forearm fracture', 'humerus fracture', 'humerus', 'shoulder fracture', 'wrist positive']

# eval dataset contains too many mistakes, will create and eval dataset by splitting the train dataset


import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os
from glob import glob
from torchvision import tv_tensors
import torchvision.transforms as transforms
import albumentations as A
from albumentations.pytorch import ToTensorV2

# Define the augmentation pipeline
def get_augmentation_pipeline():
    return A.Compose([
        #A.HorizontalFlip(p=0.5),
        #A.VerticalFlip(p=0.5),
        #A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),
        #A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.5),
        #A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2),
        A.Normalize(mean=(0.5,), std=(0.5,)),
        ToTensorV2()],
        bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])
    )

class BoneFractureDataset(Dataset):
    def __init__(self, image_dir, label_dir, transform=None):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.transform = transform
        self.image_paths = sorted(glob(os.path.join(image_dir, '*.jpg')))
        self.label_paths = sorted(glob(os.path.join(label_dir, '*.txt')))

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        #print('index:', idx)
        image_path = self.image_paths[idx]
        label_path = self.label_paths[idx]

        image = Image.open(image_path).convert("RGB")
        original_width, original_height = image.size
        image = np.array(image)

        #print(f'Image shape before: {image.shape}')
        
        boxes = []
        labels = []

        with open(label_path, 'r') as file:
            for line in file:
                parts = line.strip().split()
                class_id = int(parts[0])
                coords = [float(x) for x in parts[1:]]
                #print(f'coords: {coords}')

                # Extract the x and y coordinates
                x_coords = coords[0::2]
                y_coords = coords[1::2]

                # Convert from normalized coordinates to pixel coordinates
                x_coords = [x * original_width for x in x_coords]
                y_coords = [y * original_height for y in y_coords]

                # Find the min and max coordinates to form the bounding box
                x_min = min(x_coords)
                y_min = min(y_coords)
                x_max = max(x_coords)
                y_max = max(y_coords)

                boxes.append([x_min, y_min, x_max, y_max])
                labels.append(class_id)
        
        # Convert the bounding box coordinates to the format expected by the model

        boxes = np.array(boxes)
        
        if self.transform:
            augmented = self.transform(image=image, bboxes=boxes, labels=labels)
            image = augmented['image']
            boxes = augmented['bboxes']
            labels = augmented['labels']

            #print(f'Image shape after: {image.shape}')
            
            new_width, new_height = image.shape[2], image.shape[1]  # Get the new width and height from the transformed image

            # Scale the bounding boxes to the new image size
            scale_x = new_width / original_width
            scale_y = new_height / original_height
            boxes = [[x_min * scale_x, y_min * scale_y, x_max * scale_x, y_max * scale_y] for x_min, y_min, x_max, y_max in boxes]
        
        if len(boxes) == 0:
            # If there are no bounding boxes, create an empty tensor
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
        else:
            boxes = torch.tensor(boxes, dtype=torch.float32)
            labels = torch.tensor(labels, dtype=torch.int64)

        if len(labels) > len(boxes):
            labels = labels[:len(boxes)]


        boxes = torch.tensor(boxes, dtype=torch.float32)
 
        labels = torch.tensor(labels, dtype=torch.int64)

        image_id = torch.tensor([idx])

        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.tensor([0], dtype=torch.float32)
        if len(area) > len(boxes):
            area = area[:len(boxes)]

        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)

        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": image_id,
            "area": area,
            "iscrowd": iscrowd
        }

        return image, target


train_image_dir = '/home/mp/.cache/kagglehub/datasets/pkdarabi/bone-fracture-detection-computer-vision-project/versions/2/bone fracture detection.v4-v4.yolov8/test/images'
train_label_dir = '/home/mp/.cache/kagglehub/datasets/pkdarabi/bone-fracture-detection-computer-vision-project/versions/2/bone fracture detection.v4-v4.yolov8/test/labels'
transform = get_augmentation_pipeline()

# Split the dataset into train and eval sets
train_image_paths = sorted(glob(os.path.join(train_image_dir, '*.jpg')))
train_label_paths = sorted(glob(os.path.join(train_label_dir, '*.txt')))

# # Use train_test_split to split the paths
# train_image_paths, eval_image_paths, train_label_paths, eval_label_paths = train_test_split(
#     train_image_paths, train_label_paths, test_size=0.2, random_state=42
# )

# Create train and eval datasets
train_dataset = BoneFractureDataset(image_dir=train_image_dir, label_dir=train_label_dir, transform=transform)
train_dataset.image_paths = train_image_paths
train_dataset.label_paths = train_label_paths


import fiftyone as fo

import fiftyone.utils.coco as fouc

# Create a FiftyOne dataset
fo_dataset = fo.Dataset("bone_fracture_dataset13")

# Add samples to the dataset
for idx in range(len(train_dataset)):
    image, target = train_dataset[idx]
    image_path = train_dataset.image_paths[idx]
    
    # Convert bounding boxes to FiftyOne format
    detections = []
    for box, label in zip(target['boxes'], target['labels']):
        x_min, y_min, x_max, y_max = box.tolist()
        width = x_max - x_min
        height = y_max - y_min
        detections.append(
            fo.Detection(
                label=label_names[label.item()],
                bounding_box=[x_min / image.shape[2], y_min / image.shape[1], width / image.shape[2], height / image.shape[1]]
            )
        )
    
    # Create a FiftyOne sample
    sample = fo.Sample(filepath=image_path, ground_truth=fo.Detections(detections=detections))
    fo_dataset.add_sample(sample)

# Launch the FiftyOne app
session = fo.launch_app(fo_dataset)



